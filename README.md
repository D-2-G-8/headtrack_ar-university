# headtrack_ar

Python-пакет для трекинга головы человека в видеопотоке и наложения AR-маркеров в реальном времени.

## Описание

`headtrack_ar` — это библиотека на Python для детекции человеческих лиц, вычисления положения головы и визуализации AR-маркеров (крестиков) в области лба. Модуль полностью работает локально, не требует облачных сервисов или интернет-соединения.

## Области применения

Модуль предназначен для различных мирных приложений:

- **AR-фильтры и интерфейсы** — использование точки на лбу как якоря для наложения виртуальных объектов и эффектов
- **Авто-наведение камер** — автоматическое фокусирование и кадрирование на лице человека в видеопотоке
- **Автоматическое управление осветительными приборами** — следящее освещение для видео-конференций и стриминга
- **Образовательные приложения** — обучение правильной осанке
- **Игровые приложения** — интерактивные игры с управлением через положение головы
- **Стриминг и видеопроизводство** — автоматическое позиционирование виртуальных элементов интерфейса

## Требования

- Python 3.10 или выше
- Камера (для работы с живым видеопотоком) или видеофайл
- Операционная система: Windows, macOS, Linux

## Установка

### Установка зависимостей

```bash
pip install -r requirements.txt
```

Или установите пакет с зависимостями:

```bash
pip install -e .
```

### Локальная разработка

Для разработки с тестами:

```bash
pip install -e ".[dev]"
```

## Быстрый старт

### Использование в коде

```python
import cv2
from headtrack_ar import HeadTracker, TrackerConfig

# Создание конфигурации
config = TrackerConfig(
    source=0,                 # индекс камеры (или путь к видеофайлу)
    draw_overlay=True,       # включить отрисовку маркеров
    target_resolution=(640, 480),
)

# Инициализация трекера
tracker = HeadTracker(config)

# Обработка видеопотока
for frame_info in tracker.run():
    frame = frame_info.frame
    heads = frame_info.heads
  
    # Отображение количества обнаруженных голов
    print(f"Обнаружено голов: {len(heads)}")
  
    # Доступ к координатам точки на лбу
    for head in heads:
        forehead_x, forehead_y = head.forehead_point
        print(f"Точка на лбу: ({forehead_x}, {forehead_y})")
  
    # Отображение кадра
    cv2.imshow("HeadTrack AR", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

# Освобождение ресурсов
tracker.release()
cv2.destroyAllWindows()
```

### Запуск демо через CLI

После установки пакета можно запустить демо-приложение:

```bash
headtrack-demo
```

Или напрямую:

```bash
python -m headtrack_ar.demo
```

#### Параметры командной строки

```bash
python -m headtrack_ar.demo --help
```

Доступные опции:

- `--source`: Источник видео (индекс камеры или путь к файлу, по умолчанию: 0)
- `--width`: Ширина кадра (по умолчанию: 640)
- `--height`: Высота кадра (по умолчанию: 480)
- `--no-overlay`: Отключить отрисовку маркеров
- `--color`: Цвет маркера (green, red, blue, yellow, cyan, magenta, white, по умолчанию: green)
- `--size`: Размер маркера в пикселях (по умолчанию: 20)
- `--thickness`: Толщина линий маркера в пикселях (по умолчанию: 2)

#### Примеры использования

```bash
# Запуск с камерой по умолчанию
python -m headtrack_ar.demo

# Запуск с видеофайлом
python -m headtrack_ar.demo --source path/to/video.mp4

# Запуск с кастомными параметрами
python -m headtrack_ar.demo --source 0 --width 1280 --height 720 --color red --size 30

# Без отрисовки маркеров (только детекция)
python -m headtrack_ar.demo --no-overlay
```

## API документация

### Основные классы

#### `HeadTracker`

Главный класс для трекинга голов в видеопотоке.

```python
tracker = HeadTracker(config: TrackerConfig)
```

**Методы:**

- `run()`: Генератор, возвращающий `FrameInfo` для каждого кадра
- `process_frame(frame: np.ndarray) -> FrameInfo`: Обработка одного кадра
- `release()`: Освобождение ресурсов

#### `TrackerConfig`

Конфигурация трекера.

```python
config = TrackerConfig(
    source: Union[int, str] = 0,              # источник видео
    target_resolution: Optional[tuple] = (640, 480),
    draw_overlay: bool = True,                # отрисовка маркеров
    overlay_config: Optional[OverlayConfig] = None,
    smoothing_alpha: Optional[float] = 0.7,   # сглаживание (None = отключено)
    min_detection_confidence: float = 0.5,    # минимальная уверенность детекции
)
```

#### `HeadInfo`

Информация об обнаруженной голове.

- `bbox: tuple[int, int, int, int]` — ограничивающий прямоугольник (x, y, width, height)
- `forehead_point: tuple[int, int]` — координаты точки на лбу (x, y)
- `landmarks: Optional[list[tuple[int, int]]]` — ключевые точки лица
- `confidence: Optional[float]` — уверенность детекции (0.0 до 1.0)

#### `FrameInfo`

Информация об обработанном кадре.

- `frame: np.ndarray` — обработанный кадр (с маркерами, если включено)
- `raw_frame: Optional[np.ndarray]` — исходный кадр
- `heads: list[HeadInfo]` — список обнаруженных голов

## Производительность

Модуль оптимизирован для работы в реальном времени:

- **Целевая производительность**: 15+ FPS на среднем ноутбуке при разрешении 640x480
- **Технологии**: Использует MediaPipe для эффективной детекции лиц и ключевых точек
- **Сглаживание**: Экспоненциальное скользящее среднее для стабилизации координат

### Рекомендации по производительности

- Для лучшей производительности используйте разрешение 640x480 или ниже
- Сглаживание координат улучшает стабильность, но добавляет небольшую задержку
- Работает на CPU, поддержка GPU не требуется (но может улучшить производительность при наличии)

## Тестирование

Запуск тестов:

```bash
pytest
```

Или с покрытием:

```bash
pytest --cov=headtrack_ar
```

## Структура проекта

```
headtrack_ar/
├── __init__.py          # Экспорты пакета
├── config.py            # Конфигурация
├── types.py             # Структуры данных
├── video_source.py      # Работа с видеопотоком
├── detector.py          # Детекция лиц (MediaPipe)
├── tracker.py           # Основная логика трекинга
├── overlay.py           # Отрисовка AR-маркеров
└── demo.py              # CLI демо-приложение

tests/
└── test_basic.py        # Базовые тесты
```

## Технические детали

### Алгоритм вычисления точки на лбу

1. Детекция лица выполняется с помощью MediaPipe Face Detection
2. Извлечение ключевых точек выполняется через MediaPipe Face Mesh
3. Точка на лбу вычисляется на основе верхних ключевых точек лица
4. Применяется сглаживание координат для стабилизации (экспоненциальное скользящее среднее)

### Поддержка нескольких голов

Модуль поддерживает одновременный трекинг нескольких голов в кадре. Каждая голова получает отдельный маркер и индивидуальное сглаживание координат.

## Примечания

- Модуль полностью работает локально, без необходимости интернет-соединения
- Не требует API ключей или облачных сервисов
- Использует открытые библиотеки: OpenCV и MediaPipe
